# @package _global_

# to execute this experiment run:
# python run.py experiment=srgan.yaml

defaults:
  - override /mode: exp.yaml
  - override /trainer: null
  - override /model: null
  - override /datamodule: null
  - override /callbacks: null
  - override /logger: null

name: "semigan"

seed: 42

trainer:
  _target_: pytorch_lightning.Trainer
  gpus: 1
  min_epochs: 1
  max_epochs: 150
  gradient_clip_val: 0.8
  accumulate_grad_batches: 1
  weights_summary: "full"
  num_sanity_val_steps: 0

model:  
  _target_: src.models.semigan.SemiGAN
  discriminator_x:
    _target_: src.models.modules.dense.Fcn
    in_features: 19
    hidden_features: [500]
    out_features: 1
  discriminator_y:
    _target_: src.models.modules.dense.Fcn
    in_features: 1
    hidden_features: [500]
    out_features: 1
  discriminator_xy:
    _target_: src.models.modules.dense.Fcn
    in_features: 20  # 19 + 1 because D_xy also gets the features AND the label as input
    hidden_features: [500]
    out_features: 1
  generator_x:
    _target_: src.models.modules.dense.Fcn
    in_features: 25
    hidden_features: [16, 16, 16]
    out_features: 19
  generator_y:
    _target_: src.models.modules.dense.Fcn
    in_features: 25
    hidden_features: [16, 16, 16] 
    out_features: 1
  inverse_net:
    _target_: src.models.modules.dense.Fcn
    in_features: 19
    hidden_features: [64, 64, 64]
    out_features: 25
  inference_net:
    _target_: src.models.modules.dense.DenseNetFcn
    in_features: 19
    hidden_features: [500, 500, 500]
    out_features: 1
  dis_x_multiplier: 1.0  # λ_D_x 
  dis_y_multiplier: 1.0  # λ_D_y
  dis_xy_multiplier: 1.0  # λ_D_xy
  gen_x_multiplier: 1.0  # λ_G_x
  gen_y_multiplier: 1.0  # λ_G_y
  gen_xy_multiplier: 1.0  # λ_G_xy
  translation_multiplier: 1.0  # λ_tra
  reconstruction_multiplier: 1.0  # λ_rec
  inverse_multiplier: 1.0  # λ_inv
  synthesized_multiplier: 1.0  # λ_syn
  consistency_multiplier: 1.0  # λ_con
  lr: 0.00001
  latent_size: 25  # for the generators
  sufficient_inference_epoch: 10  

datamodule:
  _target_: src.datamodules.datamodules.ParkinsonDataModule
  data_dir: ${data_dir} # data_dir is specified in config.yaml
  batch_size: 128
  split_mode: "relative"
  split:
    - 0.8
    - 0.1
    - 0.9
  num_workers: 0
  pin_memory: False

callbacks:
  model_checkpoint:
    _target_: pytorch_lightning.callbacks.ModelCheckpoint
    monitor: "val/MSE" # name of the logged metric which determines when model is improving
    mode: "min" # "max" means higher metric value is better, can be also "min"
    save_top_k: 1 # save k best models (determined by above metric)
    save_last: True # additionaly always save model from last epoch
    verbose: False
    dirpath: "checkpoints/"
    filename: "epoch_{epoch:03d}"
    auto_insert_metric_name: False

  early_stopping:
    _target_: pytorch_lightning.callbacks.EarlyStopping
    monitor: "val/MSE" # name of the logged metric which determines when model is improving
    mode: "min" # "max" means higher metric value is better, can be also "min"
    patience: 100 # how many validation epochs of not improving until training stops
    min_delta: 0 # minimum change in the monitored metric needed to qualify as an improvement

  rich_progress_bar:
    _target_: pytorch_lightning.callbacks.RichProgressBar

#logger:
  #wandb:
    #_target_: pytorch_lightning.loggers.wandb.WandbLogger
    #project: "template-tests"
    #name: ${name}/${now:%Y-%m-%d}/${now:%H-%M-%S}
    #save_dir: "."
    #offline: False
    #log_model: false
    #prefix: ""
    #job_type: "train"
    #group: ""
    #tags: ["best_model", "skillcraft"]
    #notes: "This is an SemiGAN model test run."